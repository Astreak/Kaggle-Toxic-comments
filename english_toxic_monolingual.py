# -*- coding: utf-8 -*-
"""English_toxic_monolingual.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KFxLu9j3lEAshCWmxT_oQPI24JLjnSC4
"""

from google.colab import drive
drive.mount("/content/PRJ")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import nltk
nltk.download("stopwords")
nltk.download("punkt")
nltk.download("averaged_perceptron_tagger")
from nltk.stem import PorterStemmer,SnowballStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
ps=PorterStemmer()
data=pd.read_csv("PRJ/My Drive/hello/train.csv")
X=[]
for i in data["comment_text"]:
    X.append(i)

S=[]
L=["DT","WRB","CC","IN","PRP$","PRP","TO","NNP"]
for i in range(len(X)):
    sent=[]
    A=[]
    T=nltk.pos_tag(X[i].split())
    for j in T:
        
        if j[1] not in L:
            A.append(j[0])

    sent=" ".join(A)
    S.append(sent)
H=set(stopwords.words("english"))

H.add("tea")
H.add("father")
H.add("brother")
H.add("coffee")

H.add("about")
H.add("thing")
H.add("all")
x=["a","b","c","d","e","f","i","j","k","l","m","n","o","p","q","r","s","t","u","v",
   "w","x","y","z"]
for zz in x:
    H.add(zz)
f=[]
for i in range(len(X)):
    
    X[i]=" ".join([w.lower() for w in X[i].split()])
    X[i]=re.sub(r"[^a-z]"," ",X[i])
    D=word_tokenize(X[i])
    k=[]
    for j in D:
        if j not in H:
            k.append(j)
        else:
            continue
    f.append(" ".join(k))

token=Tokenizer(num_words=70000)
token.fit_on_texts(f)
wordi=token.word_index
seq=token.texts_to_sequences(f)
seq=pad_sequences(seq,maxlen=120,padding="post",truncating="post")
embed={}
with open("PRJ/My Drive/glove.6B.100d.txt","r",encoding="utf-8") as file:
    for line in file:
        line=line.split()
        word=line[0]
        vec=np.asarray(line[1:])
        embed[word]=vec

matrix=np.random.uniform(-0.1,0.1,size=(70000,100))
for i,j in wordi.items():
  if j>=70000:
    continue
    f=embed.get(i)
    if f is not None:
        matrix[j]=f

y=data.iloc[:,2:].values
len(seq)

from keras.models import Model
inputs=tf.keras.layers.Input(shape=(120,))
E=tf.keras.layers.Embedding(70000,100,weights=[matrix],input_length=120)
lstm1=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True,
                                                         dropout=0.2,recurrent_dropout=0.2))
lstm2=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True,
                                                        dropout=0.2,recurrent_dropout=0.2))
p=tf.keras.layers.GlobalAveragePooling1D()
d1=tf.keras.layers.Dense(64,activation="relu")
d2=tf.keras.layers.Dense(6,activation="sigmoid")

mo=tf.keras.models.Sequential()
mo.add(E)
mo.add(lstm1)
mo.add(lstm2)
mo.add(p)
mo.add(d1)
mo.add(d2)
mo.compile(optimizer="adam",loss="binary_crossentropy",metrics=["accuracy"])
mo.fit(seq,y,epochs=2,batch_size=32,validation_split=0.2)

pred=mo.predict(seq)

mo.save("Toxicity.h5")

pop="Hey brother you are a whore"
P=token.texts_to_sequences([pop])

P=pad_sequences(P,maxlen=120,padding="post")

Pred=mo.predict(P)
Pred